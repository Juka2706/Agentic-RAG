"""Local LLM wrapper for llama.cpp or vLLM."""
# TODO: load model, generate(prompt)->str
